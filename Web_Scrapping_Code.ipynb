{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#For example county = Alameda\n",
    "#https://www.redfin.com/county/303/CA/Alameda-County/new-listings\n",
    "#Fetch all the county urls\n",
    "def getCountyUrls():\n",
    "    home_page = Request('https://www.redfin.com/sitemap/CA/newest-homes', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    htmltext = urlopen(home_page).read()\n",
    "    home_page_soup = BS(htmltext,'html.parser')\n",
    "    county_urls = []\n",
    "    \n",
    "    for c_url in home_page_soup.findAll('a', attrs={'href': re.compile(\"^\\/county\")}):\n",
    "        c_url= c_url.get('href')\n",
    "        c_url= \"https://www.redfin.com\"+c_url\n",
    "        county_urls.append(c_url)\n",
    "    return (county_urls) #36 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detail Url Example - https://www.redfin.com/CA/San-Leandro/661-Lee-Ave-94577/home/762354\n",
    "#Fetch detail urls from 5 pages of each county for NewListing properties\n",
    "def getDetailUrls_NewListings(c_url_list):\n",
    "    detail_urls = []\n",
    "    \n",
    "    #https://www.redfin.com/county/303/CA/Alameda-County/new-listings\n",
    "    for c_url in c_url_list:\n",
    "        for page_no in range(1,6):\n",
    "        #for page_no in range(1,2):\n",
    "            temp_url=c_url\n",
    "            if page_no>1:\n",
    "                temp_url = temp_url+\"/page-\"+str(page_no)\n",
    "            home_page = Request(temp_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            htmltext = urlopen(home_page).read()\n",
    "            home_page_soup = BS(htmltext,'html.parser')\n",
    "        \n",
    "            for d_url in home_page_soup.findAll('a', attrs={'href': re.compile(\"^\\/CA\")}):\n",
    "                d_url = d_url.get('href')\n",
    "                d_url = \"https://www.redfin.com\"+d_url\n",
    "                if (d_url not in detail_urls):\n",
    "                    detail_urls.append(d_url)\n",
    "    return (detail_urls) #returns maximum 100 d_urls for each county (for 5 pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch detail urls from 5 pages of each county for Sold properties\n",
    "def getDetailUrls_SoldProperty(c_url_list):\n",
    "    detail_urls = []\n",
    "    \n",
    "    #https://www.redfin.com/county/303/CA/Alameda-County/new-listings\n",
    "    for c_url in c_url_list:\n",
    "        c_url = c_url+'filter/include=sold-3mo'\n",
    "        for page_no in range(1,6):\n",
    "        #for page_no in range(1,2):\n",
    "            temp_url=c_url\n",
    "            if page_no>1:\n",
    "                temp_url = temp_url+\"/page-\"+str(page_no)\n",
    "            home_page = Request(temp_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            htmltext = urlopen(home_page).read()\n",
    "            home_page_soup = BS(htmltext,'html.parser')\n",
    "        \n",
    "            for d_url in home_page_soup.findAll('a', attrs={'href': re.compile(\"^\\/CA\")}):\n",
    "                d_url = d_url.get('href')\n",
    "                d_url = \"https://www.redfin.com\"+d_url\n",
    "                if (d_url not in detail_urls):\n",
    "                    detail_urls.append(d_url)\n",
    "    return (detail_urls) #returns maximum 100 d_urls for each county (for 5 pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data_in_df_new_listings(detail_url_list):\n",
    "    address_list=[]\n",
    "    locality_list=[]\n",
    "    region_list=[]\n",
    "    postal_code_list=[]\n",
    "    price_list=[]\n",
    "    beds_list=[]\n",
    "    baths_list=[]\n",
    "    per_sq_ft_list=[]\n",
    "    area_list=[]\n",
    "    on_redfin_list=[]\n",
    "    status_list=[]\n",
    "    built_list=[]\n",
    "    type_list=[]\n",
    "    style_list=[]\n",
    "    view_list=[]\n",
    "    community_list=[]\n",
    "    mls_list=[]\n",
    "    lot_size_list=[]\n",
    "    stories_list=[]\n",
    "    county_list=[]\n",
    "    walkable_list=[]\n",
    "    walkableScore_list=[]\n",
    "    transit_list=[]\n",
    "    transitScore_list=[]\n",
    "    bikeable_list=[]\n",
    "    bikeableScore_list=[]\n",
    "    no_of_schools_list=[]\n",
    "    \n",
    "    \n",
    "    for count in range(len(detail_url_list)):\n",
    "        detail_url=detail_url_list[count]\n",
    "        detail_page = Request(detail_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        htmltext = urlopen(detail_page).read()\n",
    "        text = htmltext.decode(encoding=\"utf8\", errors='ignore')\n",
    "        detail_page_soup = BS(text,'html.parser')\n",
    "    \n",
    "        address=detail_page_soup.findAll('span', attrs={'class': \"street-address\"})\n",
    "        address_list.append(address[0].get_text() if len(address)>0 else 'NA')\n",
    "\n",
    "        locality=detail_page_soup.findAll('span', attrs={'class': \"locality\"})\n",
    "        locality_list.append(locality[0].get_text() if len(locality)>0 else 'NA')\n",
    "\n",
    "        region = detail_page_soup.findAll('span', attrs={'class': \"region\"})\n",
    "        region_list.append(region[0].get_text() if len(region)>0 else 'NA')\n",
    "\n",
    "        postal_code = detail_page_soup.findAll('span', attrs={'class': \"postal-code\"})\n",
    "        \n",
    "        postal_code = detail_page_soup.findAll('span', attrs={'class': \"postal-code\"})\n",
    "        postal_code_list.append(postal_code[0].get_text() if len([postal_code])>0 else 'NA')\n",
    "\n",
    "        price = detail_page_soup.findAll('div', attrs={'class': \"statsValue\"})\n",
    "        price_list.append(price[0].get_text() if len(price)>0 else 'NA')\n",
    "\n",
    "        beds = detail_page_soup.findAll('div', attrs={'data-rf-test-id': \"abp-beds\"})\n",
    "        beds_list.append(beds[0].get_text() if len(beds)>0 else 'NA')\n",
    "\n",
    "        baths = detail_page_soup.findAll('div', attrs={'data-rf-test-id': \"abp-baths\"})\n",
    "        baths_list.append(baths[0].get_text() if len(baths)>0 else 'NA')\n",
    "            \n",
    "        per_sq_ft = detail_page_soup.findAll('div', attrs={'data-rf-test-id': \"abp-priceperft\"})\n",
    "        per_sq_ft_list.append(per_sq_ft[0].get_text() if len(per_sq_ft)>0 else 'NA')\n",
    "\n",
    "        area = detail_page_soup.findAll('span', attrs={'class': \"statsValue\"})\n",
    "        area_list.append(area[0].get_text() if len(area)>0 else 'NA')\n",
    "        \n",
    "        #Borrowed rohit's On_Redfin logic\n",
    "        on_redfin_label = detail_page_soup.findAll('span',attrs={'class':'label'})\n",
    "        on_redfin = detail_page_soup.findAll('span',attrs={'class':'value'})\n",
    "        \n",
    "        for i in range(len(on_redfin_label)):\n",
    "            if on_redfin_label[i].string is not None:\n",
    "                if 'On Redfin' in on_redfin_label[i].string:\n",
    "                    on_redfin_list.append(on_redfin[i].get_text())\n",
    "\n",
    "        status = detail_page_soup.findAll('span', attrs={'class': \"DefinitionFlyoutLink inline-block underline clickable\"})\n",
    "        status_list.append(status[0].get_text() if len(status)>0 else 'NA')\n",
    "\n",
    "\n",
    "        keyFeatures=detail_page_soup.findAll('span', attrs={'class': \"header font-color-gray-light\"})\n",
    "        valueFeatures=detail_page_soup.findAll('span', attrs={'class': \"content text-right\"})\n",
    "        key_list=[]\n",
    "        value_list=[]\n",
    "        for count in range(len(keyFeatures)):\n",
    "            key_list.append(keyFeatures[count].get_text().lower()) #changed to Lowercase\n",
    "            value_list.append(valueFeatures[count].get_text())\n",
    "    \n",
    "        #HouseType\n",
    "        type1=['type','public details','property type'] #lowercase\n",
    "        key_index=-1\n",
    "        \n",
    "        for i in range(len(type1)):\n",
    "            if type1[i] in key_list:\n",
    "                key_index=key_list.index(type1[i])\n",
    "        \n",
    "        type_list.append(value_list[key_index] if key_index>=0 else 'NA')\n",
    "           \n",
    "        style_list.append(value_list[key_list.index('style')] if 'style' in key_list else 'NA')\n",
    "            \n",
    "        view_list.append(value_list[key_list.index('view')] if 'view' in key_list else 'NA')\n",
    "            \n",
    "        community_list.append(value_list[key_list.index('community')] if 'community' in key_list else 'NA')\n",
    "\n",
    "        county_list.append(value_list[key_list.index('county')] if 'county' in key_list else 'NA')\n",
    "\n",
    "        mls_list.append(value_list[key_list.index('mls#')] if 'mls#' in key_list else 'NA')\n",
    "\n",
    "        built_list.append(value_list[key_list.index('built')] if 'built' in key_list else 'NA')\n",
    "\n",
    "        lot_size_list.append(value_list[key_list.index('lot size')] if 'lot size' in key_list else 'NA')\n",
    "                      \n",
    "        stories_list.append(value_list[key_list.index('stories')] if 'stories' in key_list else 'NA')\n",
    "                   \n",
    "        transportMode=detail_page_soup.findAll('div', attrs={'class': \"transport-desc-and-label\"})\n",
    "        transportScore=detail_page_soup.findAll('div', attrs={'class': \"percentage\"})\n",
    "        transportKey_list=[]\n",
    "        transportValue_list=[]\n",
    "        counter=0\n",
    "        \n",
    "        for count in range(len(transportMode)):\n",
    "            transportKey_list.append(transportMode[count].get_text())\n",
    "            transportValue_list.append(transportScore[count].get_text())  \n",
    "        if any(x in transportKey_list for x in ['Very Walkable','Car-Dependent','Somewhat Walkable',\"Walker's Paradise\"]):    \n",
    "            walkable_list.append(transportKey_list[0])\n",
    "            walkableScore_list.append(transportValue_list[0])\n",
    "        else:\n",
    "            walkable_list.append('NA')\n",
    "            walkableScore_list.append('NA')\n",
    "            \n",
    "        if any(x in transportKey_list for x in ['Good Transit','Minimal Transit','Excellent Transit','Some Transit']):\n",
    "            transit_list.append(transportKey_list[1])\n",
    "            transitScore_list.append(transportValue_list[1])\n",
    "            counter=2\n",
    "        else:\n",
    "            transit_list.append('NA')\n",
    "            transitScore_list.append('NA')\n",
    "            counter=1  \n",
    "    \n",
    "        if any(x in transportKey_list for x in ['Very Bikeable','Somewhat Bikeable','Bikeable',\"Biker's Paradise\"]):    \n",
    "            bikeable_list.append(transportKey_list[counter])\n",
    "            bikeableScore_list.append(transportValue_list[counter])\n",
    "        else:\n",
    "            bikeable_list.append('NA')\n",
    "            bikeableScore_list.append('NA') \n",
    "            \n",
    "        no_of_schools=detail_page_soup.findAll('tr',attrs={'class':'schools-table-row'})\n",
    "        no_of_schools_list.append(len(no_of_schools))\n",
    "        \n",
    "    redfin_dataframe = pd.DataFrame(list(zip(address_list,locality_list,region_list,postal_code_list,price_list,beds_list,\n",
    "                                             baths_list,per_sq_ft_list,area_list,on_redfin_list,status_list,built_list,\n",
    "                                             type_list,style_list,view_list,community_list,mls_list,lot_size_list,\n",
    "                                             stories_list,county_list,walkable_list,walkableScore_list,\n",
    "                                             transit_list,transitScore_list,bikeable_list,bikeableScore_list,no_of_schools_list)),\n",
    "                                    columns=['address','locality','region','postal_code','price','beds','baths','per_sq_ft',\n",
    "                                             'area','on_redfin','status','built','type','style','view','community','mls',\n",
    "                                             'lot_size','stories','county','walkable','walkable Score','transit',\n",
    "                                             'transit score','bikeable','bikeable score','# of schools'])\n",
    "    return (redfin_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data_sold_property(detail_url_list):\n",
    "    address_list=[]\n",
    "    locality_list=[]\n",
    "    region_list=[]\n",
    "    postal_code_list=[]\n",
    "    estimate_price_list=[] #changed\n",
    "    sold_price_list=[] #changed\n",
    "    sold_date_list = [] #changed\n",
    "    beds_list=[]\n",
    "    baths_list=[]\n",
    "    per_sq_ft_list=[]\n",
    "    area_list=[]\n",
    "    on_redfin_list=[]\n",
    "    status_list=[]\n",
    "    built_list=[]\n",
    "    type_list=[]\n",
    "    style_list=[]\n",
    "    view_list=[]\n",
    "    community_list=[]\n",
    "    mls_list=[]\n",
    "    lot_size_list=[]\n",
    "    stories_list=[]\n",
    "    county_list=[]\n",
    "    walkable_list=[]\n",
    "    walkableScore_list=[]\n",
    "    transit_list=[]\n",
    "    transitScore_list=[]\n",
    "    bikeable_list=[]\n",
    "    bikeableScore_list=[]\n",
    "    no_of_schools_list=[]\n",
    "    \n",
    "    for count in range(len(detail_url_list)):\n",
    "        detail_url=detail_url_list[count]\n",
    "        detail_page = Request(detail_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        htmltext = urlopen(detail_page).read()\n",
    "        text = htmltext.decode(encoding=\"utf8\", errors='ignore')\n",
    "        detail_page_soup = BS(text,'html.parser')\n",
    "    \n",
    "        address=detail_page_soup.findAll('span', attrs={'class': \"street-address\"})\n",
    "        address_list.append(address[0].get_text() if len(address)>0 else 'NA')\n",
    "\n",
    "        locality=detail_page_soup.findAll('span', attrs={'class': \"locality\"})\n",
    "        locality_list.append(locality[0].get_text() if len(locality)>0 else 'NA')\n",
    "\n",
    "        region = detail_page_soup.findAll('span', attrs={'class': \"region\"})\n",
    "        region_list.append(region[0].get_text() if len(region)>0 else 'NA')\n",
    "        \n",
    "        postal_code = detail_page_soup.findAll('span', attrs={'class': \"postal-code\"})\n",
    "        postal_code_list.append(postal_code[0].get_text() if len([postal_code])>0 else 'NA')\n",
    "                \n",
    "        price = detail_page_soup.findAll('div', attrs={'class': \"statsValue\"})\n",
    "        estimate_price_list.append(price[0].get_text() if len(price)>0 else 'NA') #changed\n",
    "        sold_price_list.append(price[1].get_text() if len(price)>0 else 'NA') #changed\n",
    "        \n",
    "        sold_date = detail_page_soup.findAll('span',attrs={'class':'HomeSash','data-rf-test-id':'home-sash'}) #New\n",
    "        sold_date_list.append(sold_date[0].get_text()) #New\n",
    "\n",
    "        beds = detail_page_soup.findAll('div', attrs={'data-rf-test-id': \"abp-beds\"})\n",
    "        beds_list.append(beds[0].get_text() if len(beds)>0 else 'NA')\n",
    "\n",
    "        baths = detail_page_soup.findAll('div', attrs={'data-rf-test-id': \"abp-baths\"})\n",
    "        baths_list.append(baths[0].get_text() if len(baths)>0 else 'NA')\n",
    "\n",
    "        per_sq_ft = detail_page_soup.findAll('div', attrs={'data-rf-test-id': \"abp-priceperft\"})\n",
    "        per_sq_ft_list.append(per_sq_ft[0].get_text() if len(per_sq_ft)>0 else 'NA')\n",
    "\n",
    "        area = detail_page_soup.findAll('span', attrs={'class': \"statsValue\"})\n",
    "        area_list.append(area[0].get_text() if len(area)>0 else 'NA')\n",
    "    \n",
    "        built = detail_page_soup.findAll('span', attrs={'class': \"value\"})\n",
    "        on_redfin = built[2].get_text() if len(built)>2 else 'NA'\n",
    "        on_redfin_list.append(on_redfin)\n",
    "        \n",
    "        on_redfin_label = detail_page_soup.findAll('span',attrs={'class':'label'})\n",
    "        on_redfin = detail_page_soup.findAll('span',attrs={'class':'value'})\n",
    "        \n",
    "        for i in range(len(on_redfin_label)):\n",
    "            if on_redfin_label[i].string is not None:\n",
    "                if 'On Redfin' in on_redfin_label[i].string:\n",
    "                    on_redfin_list.append(on_redfin[i].get_text())\n",
    "\n",
    "        status = detail_page_soup.findAll('span', attrs={'class': \"DefinitionFlyoutLink inline-block underline clickable\"})\n",
    "        status_list.append(status[0].get_text() if len(status)>0 else 'NA')\n",
    "\n",
    "        keyFeatures=detail_page_soup.findAll('span', attrs={'class': \"header font-color-gray-light\"})\n",
    "        valueFeatures=detail_page_soup.findAll('span', attrs={'class': \"content text-right\"})\n",
    "        key_list=[]\n",
    "        value_list=[]\n",
    "        for count in range(len(keyFeatures)):\n",
    "            key_list.append(keyFeatures[count].get_text().lower()) #changed to Lowercase\n",
    "            value_list.append(valueFeatures[count].get_text())\n",
    "    \n",
    "        #HouseType\n",
    "        type1=['type','public details','property type']\n",
    "        key_index=-1\n",
    "        \n",
    "        for i in range(len(type1)):\n",
    "            if type1[i] in key_list:\n",
    "                key_index=key_list.index(type1[i])\n",
    "        \n",
    "        type_list.append(value_list[key_index] if key_index>=0 else 'NA')\n",
    "           \n",
    "        style_list.append(value_list[key_list.index('style')] if 'style' in key_list else 'NA')\n",
    "            \n",
    "        view_list.append(value_list[key_list.index('view')] if 'view' in key_list else 'NA')\n",
    "            \n",
    "        community_list.append(value_list[key_list.index('community')] if 'community' in key_list else 'NA')\n",
    "\n",
    "        county_list.append(value_list[key_list.index('county')] if 'county' in key_list else 'NA')\n",
    "\n",
    "        mls_list.append(value_list[key_list.index('mls#')] if 'mls#' in key_list else 'NA')\n",
    "\n",
    "        built_list.append(value_list[key_list.index('built')] if 'built' in key_list else 'NA')\n",
    "\n",
    "        lot_size_list.append(value_list[key_list.index('lot size')] if 'lot size' in key_list else 'NA')\n",
    "                      \n",
    "        #Stories\n",
    "        stories_list.append(value_list[key_list.index('stories')] if 'stories' in key_list else 'NA')\n",
    "\n",
    "    \n",
    "        transportMode=detail_page_soup.findAll('div', attrs={'class': \"transport-desc-and-label\"})\n",
    "        transportScore=detail_page_soup.findAll('div', attrs={'class': \"percentage\"})\n",
    "        transportKey_list=[]\n",
    "        transportValue_list=[]\n",
    "        counter=0\n",
    "        \n",
    "        for count in range(len(transportMode)):\n",
    "            transportKey_list.append(transportMode[count].get_text())\n",
    "            transportValue_list.append(transportScore[count].get_text())  \n",
    "        if any(x in transportKey_list for x in ['Very Walkable','Car-Dependent','Somewhat Walkable',\"Walker's Paradise\"]):    \n",
    "            walkable_list.append(transportKey_list[0])\n",
    "            walkableScore_list.append(transportValue_list[0])\n",
    "        else:\n",
    "            walkable_list.append('NA')\n",
    "            walkableScore_list.append('NA')\n",
    "            \n",
    "        if any(x in transportKey_list for x in ['Good Transit','Minimal Transit','Excellent Transit','Some Transit']):\n",
    "            transit_list.append(transportKey_list[1])\n",
    "            transitScore_list.append(transportValue_list[1])\n",
    "            counter=2\n",
    "        else:\n",
    "            transit_list.append('NA')\n",
    "            transitScore_list.append('NA')\n",
    "            counter=1  \n",
    "    \n",
    "        if any(x in transportKey_list for x in ['Very Bikeable','Somewhat Bikeable','Bikeable',\"Biker's Paradise\"]):    \n",
    "            bikeable_list.append(transportKey_list[counter])\n",
    "            bikeableScore_list.append(transportValue_list[counter])\n",
    "        else:\n",
    "            bikeable_list.append('NA')\n",
    "            bikeableScore_list.append('NA')\n",
    "            \n",
    "        no_of_schools=detail_page_soup.findAll('tr',attrs={'class':'schools-table-row'})\n",
    "        no_of_schools_list.append(len(no_of_schools))\n",
    "        \n",
    "    redfin_dataframe = pd.DataFrame(list(zip(address_list,locality_list,region_list,postal_code_list,estimate_price_list,\n",
    "                                             sold_price_list,sold_date_list,beds_list,\n",
    "                                             baths_list,per_sq_ft_list,area_list,on_redfin_list,status_list,built_list,\n",
    "                                             type_list,style_list,view_list,community_list,mls_list,lot_size_list,\n",
    "                                             stories_list,county_list,walkable_list,walkableScore_list,transit_list,transitScore_list,\n",
    "                                             bikeable_list,bikeableScore_list,no_of_schools_list)),\n",
    "                                    columns=['address','locality','region','postal_code','estimate_price','sold_price_list',\n",
    "                                             'sold_date_list','beds','baths','per_sq_ft','area','on_redfin','status','built','type','style','view','community','mls',\n",
    "                                             'lot_size','stories','county','walkable','walkable Score','transit','transit score','bikeable',\n",
    "                                             'bikeable score','# of schools'])\n",
    "    return (redfin_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_excel(data_frame_new,data_frame_sold):\n",
    "    writer=pd.ExcelWriter('Redfin.xlsx')\n",
    "    data_frame_new.to_excel(writer,'NewListings',index=False)\n",
    "    data_frame_sold.to_excel(writer,'SoldProperty',index=False)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 502: Bad Gateway",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4838d03e50aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#for new listings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mn_detail_url_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetDetailUrls_NewListings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounty_url_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdata_frame_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_data_in_df_new_listings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_detail_url_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5ed3edcec6dd>\u001b[0m in \u001b[0;36mgetDetailUrls_NewListings\u001b[1;34m(c_url_list)\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mtemp_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_url\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/page-\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mhome_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mhtmltext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhome_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mhome_page_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtmltext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    county_url_list = []\n",
    "    detail_url_list = []\n",
    "    \n",
    "    #returns urls for 36 counties\n",
    "    county_url_list = getCountyUrls()   \n",
    "    \n",
    "    #for new listings\n",
    "    n_detail_url_list = getDetailUrls_NewListings(county_url_list)\n",
    "    data_frame_new = scrape_data_in_df_new_listings(n_detail_url_list) #3373 Records\n",
    "    \n",
    "    #for sold property\n",
    "    s_detail_url_list = getDetailUrls_SoldProperty(county_url_list)\n",
    "    data_frame_sold = scrape_data_sold_property(s_detail_url_list) #3448 Records\n",
    "    \n",
    "    #to write the data into the excel\n",
    "    write_in_excel(data_frame_new,data_frame_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
